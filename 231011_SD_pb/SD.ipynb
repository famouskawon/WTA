{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "간단한 상황\n",
    "- Weapon : 50\n",
    "- Target : 100 (weight : 1~100)\n",
    "- 적재량 : 3\n",
    "\n",
    "- 상황 구현\n",
    "- 에이전트 구현\n",
    "\n",
    "kernel은 base\n",
    "\n",
    "목적함수 대신, 새로운 결과치를 내야 한다.\n",
    "1개 요격 시 보상인데, 오래 걸렸을 수록 감쇠된 보상을 얻어야 한다.\n",
    "그런데 위험도에 따라 보상이 달라져야겠지.\n",
    "그럼 weight 만큼의 보상을 주면 되지 않을까"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.random.seed(42)는 요격확률과 초기 위험도를 설정하게 해 줄것이다.\n",
    "\n",
    "\n",
    "random.seed(42)는 요격시 실제로 요격되는지에 설정할 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요 라이브러리 import\n",
    "import collections\n",
    "import copy\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "buffer_limit = 50000\n",
    "gamma = 0.5\n",
    "learning_rate = 0.0001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WTA World 클래스\n",
    "class WTAWorld():\n",
    "    def __init__(self,W,T,M):\n",
    "        self.W = W\n",
    "        self.T = T\n",
    "        self.M = M\n",
    "\n",
    "        # Target t에 weapon w를 할당할 때의 요격확률 생성\n",
    "        self.PK_wt = np.random.rand(W, T)\n",
    "        # Target index 할당\n",
    "        self.targets = np.array(list(range(T)))\n",
    "        # Target 별 초기 위험도 할당\n",
    "        self.init_weights = np.random.randint(1,100,size=T)\n",
    "        # Target의 현재 위험도 설정 (0 = 파괴)\n",
    "        self.weights = copy.copy(self.init_weights)\n",
    "\n",
    "\n",
    "        self.M_W = [self.M] * self.W\n",
    "        self.left_M_W = self.M_W\n",
    "        \n",
    "    def step(self, theta_wt):\n",
    "        # 값이 1인 요소를 찾기\n",
    "        indices = np.where(theta_wt == 1)\n",
    "        \n",
    "        # 좌표 형태로 출력\n",
    "        coordinates = list(zip(indices[0], indices[1]))\n",
    "\n",
    "        for coord in coordinates :\n",
    "            w,t = coord\n",
    "            if self.PK_wt[w][t] > 0 and self.left_M_W[w] > 0 :\n",
    "                pk_probability = self.PK_wt[w][t]\n",
    "                random_value = random.random()\n",
    "                if random_value <  pk_probability :\n",
    "                    # 요격 성공\n",
    "                    self.weights[coord[1]] = 0\n",
    "                else : \n",
    "                    # 요격 실패\n",
    "                    pass\n",
    "                self.left_M_W[w] -= 1\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        done = self.is_done()\n",
    "        r = sum(self.init_weights)-sum(self.weights)\n",
    "        # print(self.init_weights)\n",
    "        # print(self.weights)\n",
    "        # print(sum(self.init_weights)-sum(self.weights))\n",
    "        self.init_weights = copy.copy(self.weights)\n",
    "        return self.PK_wt*self.weights, r,  done\n",
    "\n",
    "    def is_done(self):\n",
    "        if sum(self.left_M_W) == 0 or sum(self.weights) == 0 :\n",
    "            return True\n",
    "        else :\n",
    "            return False\n",
    "        \n",
    "    def get_state(self):\n",
    "        return (self.PK_wt, self.weights, self.left_M_W)\n",
    "    \n",
    "    def reset(self):\n",
    "        # Target t에 weapon w를 할당할 때의 요격확률 생성\n",
    "        self.PK_wt = np.random.rand(self.W, self.T)\n",
    "        # Target index 할당\n",
    "        self.targets = np.array(list(range(self.T)))\n",
    "        # Target 별 초기 위험도 할당\n",
    "        self.init_weights = np.random.randint(1,100,size=self.T)\n",
    "        # Target의 현재 위험도 설정 (0 = 파괴)\n",
    "        self.weights = copy.copy(self.init_weights)\n",
    "\n",
    "        self.M_W = [self.M] * self.W\n",
    "        self.left_M_W = self.M_W\n",
    "\n",
    "        return self.PK_wt*self.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greedy Algorithm\n",
    "\n",
    "def greedy(state):\n",
    "    PK_wt = state[0]\n",
    "    targets_weights = state[1]\n",
    "    left_M_W = state[2]\n",
    "    \n",
    "    w = PK_wt.shape[0]\n",
    "    t = PK_wt.shape[1]\n",
    "    \n",
    "    # w의 발사 여부\n",
    "    m_w = [1 if m > 0 else m for m in left_M_W]\n",
    "    # 할당 여부를 나타내는 theta_wt\n",
    "    theta_wt = np.zeros((w, t), dtype=int)\n",
    "\n",
    "    flattened = (PK_wt*targets_weights).flatten()\n",
    "    sorted_indices = np.argsort(flattened)[::-1]\n",
    "    \n",
    "    i = 0\n",
    "    indices_list = []\n",
    "    while True :\n",
    "        if i == w * t :\n",
    "            break\n",
    "        v = sorted_indices[i]\n",
    "        row = v // t # Weapon\n",
    "        col = v % t # Target\n",
    "\n",
    "        if left_M_W[row] > 0 and m_w[row] == 1:\n",
    "            \n",
    "            m_w[row] -= 1\n",
    "            theta_wt[row][col] = 1\n",
    "            indices_list.append(v)\n",
    "            print(f\"weapon {row}를 target {col}에 할당, 요격확률*가중치:{flattened[v]:.2f} \")\n",
    "\n",
    "        # 할당할 수 있는 유도탄의 개수를 모두 소모했다면 종료.\n",
    "        if sum(left_M_W) == 0:\n",
    "            break\n",
    "\n",
    "        i += 1\n",
    "\n",
    "\n",
    "    return theta_wt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1번째 스텝\n",
      "weapon 3를 target 9에 할당, 요격확률*가중치:72.96 \n",
      "weapon 6를 target 9에 할당, 요격확률*가중치:71.58 \n",
      "weapon 1를 target 1에 할당, 요격확률*가중치:63.46 \n",
      "weapon 2를 target 1에 할당, 요격확률*가중치:59.56 \n",
      "weapon 0를 target 9에 할당, 요격확률*가중치:55.82 \n",
      "weapon 8를 target 2에 할당, 요격확률*가중치:55.77 \n",
      "weapon 5를 target 1에 할당, 요격확률*가중치:50.95 \n",
      "weapon 7를 target 7에 할당, 요격확률*가중치:47.67 \n",
      "weapon 4를 target 9에 할당, 요격확률*가중치:44.80 \n",
      "weapon 9를 target 1에 할당, 요격확률*가중치:32.11 \n",
      "------발사------\n",
      "------요격결과------\n",
      "[ 6  0  0 33 42 56  9  0  1  0]\n",
      "보상: 277\n",
      "2번째 스텝\n",
      "weapon 3를 target 5에 할당, 요격확률*가중치:47.04 \n",
      "weapon 7를 target 5에 할당, 요격확률*가중치:44.54 \n",
      "weapon 4를 target 5에 할당, 요격확률*가중치:37.45 \n",
      "weapon 1를 target 5에 할당, 요격확률*가중치:37.06 \n",
      "weapon 5를 target 3에 할당, 요격확률*가중치:32.50 \n",
      "weapon 2를 target 5에 할당, 요격확률*가중치:31.40 \n",
      "weapon 0를 target 4에 할당, 요격확률*가중치:30.49 \n",
      "weapon 6를 target 5에 할당, 요격확률*가중치:28.15 \n",
      "weapon 9를 target 5에 할당, 요격확률*가중치:12.04 \n",
      "weapon 8를 target 6에 할당, 요격확률*가중치:8.90 \n",
      "------발사------\n",
      "------요격결과------\n",
      "[6 0 0 0 0 0 0 0 1 0]\n",
      "보상: 140\n",
      "3번째 스텝\n",
      "weapon 1를 target 0에 할당, 요격확률*가중치:5.38 \n",
      "weapon 6를 target 0에 할당, 요격확률*가중치:5.36 \n",
      "weapon 0를 target 0에 할당, 요격확률*가중치:4.64 \n",
      "weapon 9를 target 0에 할당, 요격확률*가중치:4.48 \n",
      "weapon 2를 target 0에 할당, 요격확률*가중치:4.43 \n",
      "weapon 8를 target 0에 할당, 요격확률*가중치:4.42 \n",
      "weapon 5를 target 0에 할당, 요격확률*가중치:4.01 \n",
      "weapon 7를 target 0에 할당, 요격확률*가중치:3.86 \n",
      "weapon 4를 target 0에 할당, 요격확률*가중치:1.99 \n",
      "weapon 3를 target 0에 할당, 요격확률*가중치:1.66 \n",
      "------발사------\n",
      "------요격결과------\n",
      "[0 0 0 0 0 0 0 0 1 0]\n",
      "보상: 6\n",
      "리턴: 348.5\n"
     ]
    }
   ],
   "source": [
    "wta=WTAWorld(10,10,3)\n",
    "step_i = 1\n",
    "G = 0\n",
    "while True : \n",
    "    print(f\"{step_i}번째 스텝\")\n",
    "    theta_wt =greedy(wta.get_state())\n",
    "    print(\"------발사------\")\n",
    "    s_prime, r, done = wta.step(theta_wt)\n",
    "    print(\"------요격결과------\")\n",
    "    print(wta.get_state()[1])\n",
    "    print(f\"보상: {r}\")\n",
    "    G += r  * gamma**(step_i-1)\n",
    "    if done:\n",
    "        break\n",
    "    step_i+=1\n",
    "\n",
    "print(f\"리턴: {G}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    env = WTAWorld(10,10,3)\n",
    "    step_i = 1\n",
    "    while True : \n",
    "        print(f\"{step_i}번째 스텝\")\n",
    "        theta_wt = greedy(env.get_state())\n",
    "        print(\"------발사------\")\n",
    "        env.step(theta_wt)\n",
    "        print(\"------요격결과------\")\n",
    "        print(wta.get_state()[0][1])\n",
    "        if env.is_done():\n",
    "            break\n",
    "        step_i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self):\n",
    "        self.buffer = collections.deque(maxlen=buffer_limit)\n",
    "    \n",
    "    def put(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "        \n",
    "    def sample(self, n):\n",
    "        mini_batch = random.sample(self.buffer, n)\n",
    "        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n",
    "        \n",
    "        for transition in mini_batch:\n",
    "            s, a, r, s_prime, done_mask = transition\n",
    "            s_lst.append(s)\n",
    "            a_lst.append(a)\n",
    "            r_lst.append([r])\n",
    "            s_prime_lst.append(s_prime)\n",
    "            done_mask_lst.append([done_mask])\n",
    "        \n",
    "        # print(torch.tensor(s_lst, dtype=torch.float))\n",
    "        # print(torch.stack(a_lst))\n",
    "        # print(torch.tensor(r_lst))\n",
    "        # print(torch.tensor(s_prime_lst, dtype=torch.float))\n",
    "        # print(torch.tensor(done_mask_lst))\n",
    "        return torch.tensor(s_lst, dtype=torch.float), torch.stack(a_lst), torch.tensor(r_lst), torch.tensor(s_prime_lst, dtype=torch.float), torch.tensor(done_mask_lst)\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qnet(nn.Module):\n",
    "    def __init__(self, W, T):\n",
    "        super(Qnet, self).__init__()\n",
    "        self.W = W\n",
    "        self.T = T\n",
    "        self.fc1 = nn.Linear(self.W*self.T, self.W*self.T*20)\n",
    "        self.fc2 = nn.Linear(self.W*self.T*20, self.W*self.T*20)\n",
    "        self.fc3 = nn.Linear(self.W*self.T*20, self.W*self.T)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "    \n",
    "    def sample_action(self, obs, epsilon):\n",
    "        out = self.forward(obs)\n",
    "        # print(out)\n",
    "        # print(out.shape)\n",
    "        reshaped_out = out.reshape(self.W,self.T)\n",
    "        \n",
    "        one_hot = torch.zeros_like(reshaped_out)\n",
    "        \n",
    "        coin = random.random()\n",
    "        if coin < epsilon:\n",
    "            random_indices = torch.randint(0, reshaped_out.shape[1], (reshaped_out.shape[0],))\n",
    "            one_hot[torch.arange(reshaped_out.shape[0]), random_indices] = 1\n",
    "            return one_hot\n",
    "        else :\n",
    "            max_indices = torch.argmax(reshaped_out, dim=1)\n",
    "            one_hot[torch.arange(reshaped_out.shape[0]), max_indices] = 1\n",
    "            return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(q, q_target, memory, optimizer):\n",
    "    for i in range(10):\n",
    "        s, a, r, s_prime, done_mask = memory.sample(batch_size)\n",
    "        q_out = q(s.flatten(start_dim=1))\n",
    "        # print(a.flatten(start_dim=1).shape)\n",
    "        # print(q_out.shape)\n",
    "        q_a = a.flatten(start_dim=1) * q_out\n",
    "        max_q_prime = q_target(s_prime.flatten(start_dim=1)).max(1)[0].unsqueeze(1)\n",
    "        target = r + gamma * max_q_prime * done_mask\n",
    "        # print(target)\n",
    "        # print(target.shape)\n",
    "        loss = F.smooth_l1_loss(q_a, target)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    env = WTAWorld(10,10,3)\n",
    "    q = Qnet(env.W, env.T)\n",
    "    q_target = Qnet(env.W, env.T)\n",
    "    q_target.load_state_dict(q.state_dict())\n",
    "    memory = ReplayBuffer()\n",
    "    \n",
    "    print_interval = 20\n",
    "    score = 0.0\n",
    "    optimizer = optim.Adam(q.parameters(), lr = learning_rate)\n",
    "    \n",
    "    for n_epi in range(100000):\n",
    "        epsilon = max(0.01, 0.08, - 0.01*(n_epi/200))\n",
    "        s = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            a = q.sample_action(torch.from_numpy(s.flatten()).float(), epsilon)\n",
    "            s_prime, r, done = env.step(a)\n",
    "            # print(a.shape)\n",
    "            # print(s_prime.shape)\n",
    "            done_mask = 0.0 if done else 1.0\n",
    "            memory.put((s, a, r, s_prime, done_mask))\n",
    "            s = s_prime\n",
    "            score += r\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        if memory.size()>2000:\n",
    "            train(q, q_target, memory, optimizer)\n",
    "        \n",
    "        if n_epi%print_interval == 0 and n_epi !=0:\n",
    "            q_target.load_state_dict(q.state_dict())\n",
    "            print(f\"n_episode: {n_epi}, score: {(score/print_interval):.1f}, n_buffer: {memory.size()}, eps: {(epsilon*100):.1f}%\")\n",
    "            score = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_episode: 20, score: 376.8, n_buffer: 63, eps: 8.0%\n",
      "n_episode: 40, score: 340.4, n_buffer: 123, eps: 8.0%\n",
      "n_episode: 60, score: 302.1, n_buffer: 182, eps: 8.0%\n",
      "n_episode: 80, score: 370.1, n_buffer: 242, eps: 8.0%\n",
      "n_episode: 100, score: 396.6, n_buffer: 302, eps: 8.0%\n",
      "n_episode: 120, score: 376.4, n_buffer: 362, eps: 8.0%\n",
      "n_episode: 140, score: 379.9, n_buffer: 421, eps: 8.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_episode: 160, score: 356.4, n_buffer: 481, eps: 8.0%\n",
      "n_episode: 180, score: 346.1, n_buffer: 541, eps: 8.0%\n",
      "n_episode: 200, score: 361.4, n_buffer: 601, eps: 8.0%\n",
      "n_episode: 220, score: 375.1, n_buffer: 661, eps: 8.0%\n",
      "n_episode: 240, score: 377.6, n_buffer: 721, eps: 8.0%\n",
      "n_episode: 260, score: 398.6, n_buffer: 781, eps: 8.0%\n",
      "n_episode: 280, score: 366.1, n_buffer: 841, eps: 8.0%\n",
      "n_episode: 300, score: 370.9, n_buffer: 901, eps: 8.0%\n",
      "n_episode: 320, score: 344.5, n_buffer: 961, eps: 8.0%\n",
      "n_episode: 340, score: 344.0, n_buffer: 1021, eps: 8.0%\n",
      "n_episode: 360, score: 368.1, n_buffer: 1081, eps: 8.0%\n",
      "n_episode: 380, score: 379.6, n_buffer: 1141, eps: 8.0%\n",
      "n_episode: 400, score: 377.3, n_buffer: 1201, eps: 8.0%\n",
      "n_episode: 420, score: 327.2, n_buffer: 1261, eps: 8.0%\n",
      "n_episode: 440, score: 360.4, n_buffer: 1321, eps: 8.0%\n",
      "n_episode: 460, score: 357.8, n_buffer: 1381, eps: 8.0%\n",
      "n_episode: 480, score: 325.9, n_buffer: 1441, eps: 8.0%\n",
      "n_episode: 500, score: 358.5, n_buffer: 1501, eps: 8.0%\n",
      "n_episode: 520, score: 353.0, n_buffer: 1561, eps: 8.0%\n",
      "n_episode: 540, score: 386.6, n_buffer: 1621, eps: 8.0%\n",
      "n_episode: 560, score: 345.0, n_buffer: 1681, eps: 8.0%\n",
      "n_episode: 580, score: 320.2, n_buffer: 1741, eps: 8.0%\n",
      "n_episode: 600, score: 388.1, n_buffer: 1801, eps: 8.0%\n",
      "n_episode: 620, score: 344.3, n_buffer: 1861, eps: 8.0%\n",
      "n_episode: 640, score: 379.4, n_buffer: 1921, eps: 8.0%\n",
      "n_episode: 660, score: 372.3, n_buffer: 1981, eps: 8.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2584907/3681971597.py:12: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 100])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.smooth_l1_loss(q_a, target)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_episode: 680, score: 281.1, n_buffer: 2041, eps: 8.0%\n",
      "n_episode: 700, score: 137.7, n_buffer: 2101, eps: 8.0%\n",
      "n_episode: 720, score: 142.2, n_buffer: 2161, eps: 8.0%\n",
      "n_episode: 740, score: 118.3, n_buffer: 2221, eps: 8.0%\n",
      "n_episode: 760, score: 106.0, n_buffer: 2281, eps: 8.0%\n",
      "n_episode: 780, score: 95.1, n_buffer: 2341, eps: 8.0%\n",
      "n_episode: 800, score: 88.2, n_buffer: 2401, eps: 8.0%\n",
      "n_episode: 820, score: 105.0, n_buffer: 2461, eps: 8.0%\n",
      "n_episode: 840, score: 111.0, n_buffer: 2521, eps: 8.0%\n",
      "n_episode: 860, score: 103.8, n_buffer: 2581, eps: 8.0%\n",
      "n_episode: 880, score: 114.8, n_buffer: 2641, eps: 8.0%\n",
      "n_episode: 900, score: 88.0, n_buffer: 2701, eps: 8.0%\n",
      "n_episode: 920, score: 211.6, n_buffer: 2761, eps: 8.0%\n",
      "n_episode: 940, score: 146.6, n_buffer: 2821, eps: 8.0%\n",
      "n_episode: 960, score: 154.4, n_buffer: 2881, eps: 8.0%\n",
      "n_episode: 980, score: 94.4, n_buffer: 2941, eps: 8.0%\n",
      "n_episode: 1000, score: 172.0, n_buffer: 3001, eps: 8.0%\n",
      "n_episode: 1020, score: 97.5, n_buffer: 3061, eps: 8.0%\n",
      "n_episode: 1040, score: 91.3, n_buffer: 3121, eps: 8.0%\n",
      "n_episode: 1060, score: 87.0, n_buffer: 3181, eps: 8.0%\n",
      "n_episode: 1080, score: 95.3, n_buffer: 3241, eps: 8.0%\n",
      "n_episode: 1100, score: 70.3, n_buffer: 3301, eps: 8.0%\n",
      "n_episode: 1120, score: 70.5, n_buffer: 3361, eps: 8.0%\n",
      "n_episode: 1140, score: 119.7, n_buffer: 3421, eps: 8.0%\n",
      "n_episode: 1160, score: 80.5, n_buffer: 3481, eps: 8.0%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/ssd2/kawon/230727_WTA/231011_SD_pb/SD.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bserver10/ssd2/kawon/230727_WTA/231011_SD_pb/SD.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m main()\n",
      "\u001b[1;32m/ssd2/kawon/230727_WTA/231011_SD_pb/SD.ipynb Cell 13\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bserver10/ssd2/kawon/230727_WTA/231011_SD_pb/SD.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bserver10/ssd2/kawon/230727_WTA/231011_SD_pb/SD.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39mif\u001b[39;00m memory\u001b[39m.\u001b[39msize()\u001b[39m>\u001b[39m\u001b[39m2000\u001b[39m:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bserver10/ssd2/kawon/230727_WTA/231011_SD_pb/SD.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m     train(q, q_target, memory, optimizer)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bserver10/ssd2/kawon/230727_WTA/231011_SD_pb/SD.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mif\u001b[39;00m n_epi\u001b[39m%\u001b[39mprint_interval \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m n_epi \u001b[39m!=\u001b[39m\u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bserver10/ssd2/kawon/230727_WTA/231011_SD_pb/SD.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m     q_target\u001b[39m.\u001b[39mload_state_dict(q\u001b[39m.\u001b[39mstate_dict())\n",
      "\u001b[1;32m/ssd2/kawon/230727_WTA/231011_SD_pb/SD.ipynb Cell 13\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bserver10/ssd2/kawon/230727_WTA/231011_SD_pb/SD.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bserver10/ssd2/kawon/230727_WTA/231011_SD_pb/SD.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     s, a, r, s_prime, done_mask \u001b[39m=\u001b[39m memory\u001b[39m.\u001b[39msample(batch_size)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bserver10/ssd2/kawon/230727_WTA/231011_SD_pb/SD.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     q_out \u001b[39m=\u001b[39m q(s\u001b[39m.\u001b[39mflatten(start_dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bserver10/ssd2/kawon/230727_WTA/231011_SD_pb/SD.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m# print(a.flatten(start_dim=1).shape)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bserver10/ssd2/kawon/230727_WTA/231011_SD_pb/SD.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m# print(q_out.shape)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bserver10/ssd2/kawon/230727_WTA/231011_SD_pb/SD.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     q_a \u001b[39m=\u001b[39m a\u001b[39m.\u001b[39mflatten(start_dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m q_out\n",
      "File \u001b[0;32m~/anaconda3/envs/231017_rl_greedy/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/231017_rl_greedy/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/ssd2/kawon/230727_WTA/231011_SD_pb/SD.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bserver10/ssd2/kawon/230727_WTA/231011_SD_pb/SD.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bserver10/ssd2/kawon/230727_WTA/231011_SD_pb/SD.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc1(x))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bserver10/ssd2/kawon/230727_WTA/231011_SD_pb/SD.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc2(x))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bserver10/ssd2/kawon/230727_WTA/231011_SD_pb/SD.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msigmoid(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc3(x))\n",
      "File \u001b[0;32m~/anaconda3/envs/231017_rl_greedy/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/231017_rl_greedy/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/231017_rl_greedy/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mlinear(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/ssd2/kawon/230727_WTA/231011_SD_pb/SD.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bserver10/ssd2/kawon/230727_WTA/231011_SD_pb/SD.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m a\u001b[39m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "230924_milp_wta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
